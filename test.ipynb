{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "232f44b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/homebrew/anaconda3/envs/taix-ray/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2696: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores:  [0.9994982481002808, 0.9993619322776794]\n",
      "anti_scores:  [0.00014956131053622812, 0.00020937531371600926]\n"
     ]
    }
   ],
   "source": [
    "# Requires transformers>=4.51.0\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def format_instruction(instruction, query, doc):\n",
    "    if instruction is None:\n",
    "        instruction = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "    output = \"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\".format(instruction=instruction,query=query, doc=doc)\n",
    "    return output\n",
    "\n",
    "def process_inputs(pairs):\n",
    "    inputs = tokenizer(\n",
    "        pairs, padding=False, truncation='longest_first',\n",
    "        return_attention_mask=False, max_length=max_length - len(prefix_tokens) - len(suffix_tokens)\n",
    "    )\n",
    "    for i, ele in enumerate(inputs['input_ids']):\n",
    "        inputs['input_ids'][i] = prefix_tokens + ele + suffix_tokens\n",
    "    inputs = tokenizer.pad(inputs, padding=True, return_tensors=\"pt\", max_length=max_length)\n",
    "    for key in inputs:\n",
    "        inputs[key] = inputs[key].to(model.device)\n",
    "    return inputs\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_logits(inputs, **kwargs):\n",
    "    batch_scores = model(**inputs).logits[:, -1, :]\n",
    "    true_vector = batch_scores[:, token_true_id]\n",
    "    false_vector = batch_scores[:, token_false_id]\n",
    "    batch_scores = torch.stack([false_vector, true_vector], dim=1)\n",
    "    batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)\n",
    "    scores = batch_scores[:, 1].exp().tolist()\n",
    "    return scores\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\", padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\").eval()\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving.\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-Reranker-4B\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").cuda().eval()\n",
    "\n",
    "token_false_id = tokenizer.convert_tokens_to_ids(\"no\")\n",
    "token_true_id = tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "max_length = 8192\n",
    "\n",
    "prefix = \"<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n<|im_start|>user\\n\"\n",
    "suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "prefix_tokens = tokenizer.encode(prefix, add_special_tokens=False)\n",
    "suffix_tokens = tokenizer.encode(suffix, add_special_tokens=False)\n",
    "        \n",
    "task = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "\n",
    "queries = [\"What is the capital of China?\",\n",
    "    \"Explain gravity\",\n",
    "]\n",
    "\n",
    "documents = [\n",
    "    \"The capital of China is Beijing.\",\n",
    "    \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\",\n",
    "]\n",
    "documents_neg = [\n",
    "    \"The capital of France is Paris.\",\n",
    "    \"Photosynthesis is the process by which green plants and some other organisms use sunlight to synthesize foods from carbon dioxide and water.\",\n",
    "]\n",
    "\n",
    "pairs = [format_instruction(task, query, doc) for query, doc in zip(queries, documents)]\n",
    "anti_pairs = [format_instruction(task, query, doc) for query, doc in zip(queries, documents_neg)]\n",
    "# Tokenize the input texts\n",
    "inputs = process_inputs(pairs)\n",
    "scores = compute_logits(inputs)\n",
    "anti_inputs = process_inputs(anti_pairs)\n",
    "anti_scores = compute_logits(anti_inputs)\n",
    "\n",
    "print(\"scores: \", scores)\n",
    "print(\"anti_scores: \", anti_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1c3ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called tool calculate with args {'b': 5, 'operator': '+', 'a': 10}\n",
      "Got result: The sum of 10 and 5 is 15.\n",
      "Called tool answer_question with args {'question': 'What is the capital of France?'}\n",
      "Got result: Paris\n",
      "15\n",
      "\n",
      "Paris\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "\n",
    "\n",
    "GOOGLE_API_KEY = \"AIzaSyC-LwOBfcEiBmrVHwYxYW0NVzUdoC1GEqM\"\n",
    "lm = dspy.LM('gemini/gemini-2.5-pro-preview-03-25', api_key=GOOGLE_API_KEY)\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "qa = dspy.ChainOfThought('question -> answer')\n",
    "\n",
    "def answer_question(question: str) -> str:\n",
    "    \"\"\"Answers a question based on general knowledge.\"\"\"\n",
    "    response = qa(question=question)\n",
    "    return response.answer\n",
    "\n",
    "def calculate(a: float, b: float, operator: str) -> float:\n",
    "    \"\"\"Simple calculus on two numbers a and b. Supports add, subtract, multiply and divide. The operator is only allowed to be one of the following: +, -, *, /. For instance, useful to estimate the development of a disease over time.\"\"\"\n",
    "    match operator:\n",
    "        case \"+\":\n",
    "            return f\"The sum of {a} and {b} is {a+b}.\"\n",
    "        case \"-\":\n",
    "            return f\"Subtracting {a} and {b} (a-b) is {a-b}.\"\n",
    "        case \"*\":\n",
    "            return f\"Multiplying {a} and {b} is {a*b}.\"\n",
    "        case \"/\":\n",
    "            return f\"The ratio between {a} and {b} is {a/b}.\"\n",
    "        case _:\n",
    "            return \"Invalid operator. Please use one of the following: +, -, *, /.\"\n",
    "\n",
    "# Wrap your functions as tools\n",
    "llm = GoogleGenAI(model=\"gemini-2.5-pro-preview-03-25\", api_key=GOOGLE_API_KEY)\n",
    "\n",
    "agent = FunctionAgent(\n",
    "    tools=[answer_question, calculate],\n",
    "    llm=llm,\n",
    ")\n",
    "from llama_index.core.agent.workflow import ToolCallResult\n",
    "async def run_agent_verbose(query: str):\n",
    "    handler = agent.run(query)\n",
    "    async for event in handler.stream_events():\n",
    "        if isinstance(event, ToolCallResult):\n",
    "            print(\n",
    "                f\"Called tool {event.tool_name} with args {event.tool_kwargs}\\nGot result: {event.tool_output}\"\n",
    "            )\n",
    "\n",
    "    return await handler\n",
    "\n",
    "\n",
    "result = await run_agent_verbose(\"Calculate 10 + 5\")\n",
    "result2 = await run_agent_verbose(\"What is the capital of France?\")\n",
    "print(result)\n",
    "print(result2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents (Python 3.11.14)",
   "language": "python",
   "name": "agents"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
